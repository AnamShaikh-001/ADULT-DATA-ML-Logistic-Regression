{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "421ac65c",
   "metadata": {},
   "source": [
    "# ADULT DATA - Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e442dc1b",
   "metadata": {},
   "source": [
    "Prediction task is to determine whether a person makes over 50K a year."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6214d4ae",
   "metadata": {},
   "source": [
    "Listing of attributes:\n",
    "\n",
    ">50K, <=50K.\n",
    "\n",
    "age: continuous<br>\n",
    "\n",
    "workclass: Private, Self-emp-not-inc, Self-emp-inc, Federal-gov, Local-gov, State-gov, Without-pay, Never-worked.<br>\n",
    "\n",
    "fnlwgt: continuous.<br>\n",
    "\n",
    "education: Bachelors, Some-college, 11th, HS-grad, Prof-school, Assoc-acdm, Assoc-voc, 9th, 7th-8th, 12th, Masters, 1st-4th, 10th, Doctorate, 5th-6th, Preschool.<br>\n",
    "\n",
    "education-num: continuous.<br>\n",
    "marital-status: Married-civ-spouse, Divorced, Never-married, Separated, Widowed, Married-spouse-absent, Married-AF-spouse.<br>\n",
    "\n",
    "occupation: Tech-support, Craft-repair, Other-service, Sales, Exec-managerial, Prof-specialty, Handlers-cleaners, Machine-op-inspct, Adm-clerical, Farming-fishing, Transport-moving, Priv-house-serv, Protective-serv, Armed-Forces.<br>\n",
    "\n",
    "relationship: Wife, Own-child, Husband, Not-in-family, Other-relative, Unmarried.<br>\n",
    "\n",
    "race: White, Asian-Pac-Islander, Amer-Indian-Eskimo, Other, Black.<br>\n",
    "sex: Female, Male.<br>\n",
    "\n",
    "capital-gain: continuous.<br>\n",
    "\n",
    "capital-loss: continuous.<br>\n",
    "\n",
    "hours-per-week: continuous.<br>\n",
    "\n",
    "native-country: United-States, Cambodia, England, Puerto-Rico, Canada, Germany, Outlying-US(Guam-USVI-etc), India, Japan, Greece, South, China, Cuba, Iran, Honduras, Philippines, Italy, Poland, Jamaica, Vietnam, Mexico, Portugal, Ireland, France, Dominican-Republic, Laos, Ecuador, Taiwan, Haiti, Columbia, Hungary, Guatemala, Nicaragua, Scotland, Thailand, Yugoslavia, El-Salvador, Trinadad&Tobago, Peru, Hong, Holand-Netherlands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f47acc7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40bdd669",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\Anny\\\\OneDrive\\\\Desktop\\\\Imarticus\\\\GITHUB\\\\Adult\\\\adult_data (1).csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_21900\\343294807.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Load the data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0madult_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr\"C:\\Users\\Anny\\OneDrive\\Desktop\\Imarticus\\GITHUB\\Adult\\adult_data (1).csv\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdelimiter\u001b[0m \u001b[1;33m=\u001b[0m\u001b[1;34m' *, *'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0madult_df\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m                 )\n\u001b[1;32m--> 311\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    676\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 678\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    679\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    680\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    573\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    574\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 575\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    576\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    577\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    930\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    931\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[1;33m|\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 932\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    933\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    934\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1214\u001b[0m             \u001b[1;31m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1215\u001b[0m             \u001b[1;31m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1216\u001b[1;33m             self.handles = get_handle(  # type: ignore[call-overload]\n\u001b[0m\u001b[0;32m   1217\u001b[0m                 \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1218\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    784\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;34m\"b\"\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    785\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 786\u001b[1;33m             handle = open(\n\u001b[0m\u001b[0;32m    787\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    788\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\Anny\\\\OneDrive\\\\Desktop\\\\Imarticus\\\\GITHUB\\\\Adult\\\\adult_data (1).csv'"
     ]
    }
   ],
   "source": [
    "# Load the data \n",
    "\n",
    "adult_df = pd.read_csv(r\"C:\\Users\\Anny\\OneDrive\\Desktop\\Imarticus\\GITHUB\\Adult\\adult_data (1).csv\", header=None, delimiter =' *, *')\n",
    "adult_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5db0cb9",
   "metadata": {},
   "source": [
    "* Since the data has missing headers, we have inserted them manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4a2fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "adult_df.columns = ['age', 'workclass', 'fnlwgt', 'education', 'education_num',\n",
    "                    'marital_status', 'occupation', 'relationship',\n",
    "                    'race', 'sex', 'capital_gain', 'capital_loss',\n",
    "                    'hours_per_week', 'native_country', 'income']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7728bb7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calling the data to check if the headers have been updated in the data\n",
    "\n",
    "adult_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0fac3e8",
   "metadata": {},
   "source": [
    "* The data consists of numeric as well as categoric data\n",
    "* Looking at the variables we can infer that:\n",
    "1. The columns 'education' and 'education_num' mean they same but in different data types<br>\n",
    "Hence we will drop any one of them\n",
    "2. The column 'fnlwgt' is a calculated variable and does not hold as much importance<br>\n",
    "Hence we will drop the column as a part of feature selection\n",
    "3. The columns 'capital_gain' and 'capital_loss' have many values that are equal to 0\n",
    "4. The columns 'relationship' and 'race' might not seem as important, but we will keep it as of now\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7741505",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General description of the data\n",
    "\n",
    "print('Data shape -', adult_df.shape)\n",
    "print()\n",
    "print('Data types','\\n', adult_df.dtypes)\n",
    "print()\n",
    "print('Description','\\n')\n",
    "print(adult_df.describe(include='all'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2080c1e8",
   "metadata": {},
   "source": [
    "* There are 32561 rows and 15 columns\n",
    "* The data consists of mixed data types - int64 and object"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a485475",
   "metadata": {},
   "source": [
    "#### Processing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "738bddba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a copy of the data \n",
    "\n",
    "adult_df_rev = pd.DataFrame.copy(adult_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b66328e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "adult_df_rev.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9984807",
   "metadata": {},
   "outputs": [],
   "source": [
    "adult_df_rev.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc14577e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping columns which are irrelevent for the model\n",
    "# Dropping 'fnlwgt' and 'education'\n",
    "\n",
    "adult_df_rev.drop(['fnlwgt', 'education'],axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa4dccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "adult_df_rev.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d251f80",
   "metadata": {},
   "source": [
    "* The number of columns have been reduced from 15 --> 13 \n",
    "* Which means column 'fnlwgt' and 'education' have been successfully dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1fef098",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for missing values in the data \n",
    "\n",
    "adult_df_rev.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf20fb33",
   "metadata": {},
   "source": [
    "* Here we can see that there is no missing values in the data\n",
    "* But, reading the data description we come to know that the data has missing values in the form of '?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28112f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the datatypes to see if the numeric variables have any anomaly in the data type\n",
    "\n",
    "adult_df_rev.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee6e7ad",
   "metadata": {},
   "source": [
    "Since there are no further anomalies in the data we can go ahead and check for special characters in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac99e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for special chareter (?) in the data\n",
    "\n",
    "for i in adult_df_rev.columns:\n",
    "    print({i:adult_df_rev[i].unique()})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0944af4",
   "metadata": {},
   "source": [
    "We can infer that columns 'workclass', 'occupation' and 'native_country' have special charecter '?' in place of null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8cb16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for duplicates in the data\n",
    "\n",
    "adult_df_rev.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e704fcb4",
   "metadata": {},
   "source": [
    "There are 3465 duplicated data<br>\n",
    "Since we have enough data we can go ahead and drop all the duplicated values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d98c41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping the duplicated values\n",
    "\n",
    "adult_df_rev.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6fb4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking if the duplicate values have been dropped\n",
    "\n",
    "adult_df_rev.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c10a10",
   "metadata": {},
   "source": [
    "Since the record have been reduced from 32561 ---> 29096<br>\n",
    "Which means that the duplicate values have been successfully dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee2b90b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing the special charecter '?' with nan\n",
    "\n",
    "adult_df_rev.replace('?',np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca3a5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking if the special charecter '?' has been replaced \n",
    "\n",
    "adult_df_rev.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1298e4c6",
   "metadata": {},
   "source": [
    "We can infer that columns 'workclass', 'occupation' and 'native_country' have missing values in their data<br>\n",
    "To avoid data loss we will fill these missing values\n",
    "\n",
    "Here we can see that \n",
    "* Workclass has 1632 missing values\n",
    "* Occupation has 1639 missing values\n",
    "* Native_country has 580 missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a66b6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filling the missing values with the mode - since the columns are categorical in nature\n",
    "\n",
    "for value in adult_df_rev.columns:\n",
    "    adult_df_rev[value].fillna(adult_df_rev[value].mode()[0],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b9ec45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking if the the missing values have been filled\n",
    "\n",
    "adult_df_rev.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3115f163",
   "metadata": {},
   "source": [
    "We can conclude that the missing values have been successfully filled with their respective modes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7046b15",
   "metadata": {},
   "source": [
    "#### Pre-processing the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c775eb",
   "metadata": {},
   "source": [
    "ASSUMPTION 1 - There should be no outliers in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7909998",
   "metadata": {},
   "outputs": [],
   "source": [
    "adult_df_rev.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2162c58b",
   "metadata": {},
   "outputs": [],
   "source": [
    "adult_num = ['age','education_num','capital_gain','capital_loss','hours_per_week']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a793fd31",
   "metadata": {},
   "outputs": [],
   "source": [
    "adult_df_rev.boxplot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d475944",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in adult_num:\n",
    "    adult_df_rev.boxplot(column=i)\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7b1356",
   "metadata": {},
   "source": [
    "We can observe that there are a few outliers in the data<br>\n",
    "But we also observe that outliers are clustered in nature so elimination of any outlier will lead to data loss\n",
    "\n",
    "Hence we will ignore the outliers<Br>\n",
    "The 1st assumption has been successfully met"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec117df",
   "metadata": {},
   "source": [
    "#### **Pre processing the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce617307",
   "metadata": {},
   "outputs": [],
   "source": [
    "adult_df_rev.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ff611d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting all categoric data into numeric data\n",
    "\n",
    "# Creating a list that has only values with 'object' datatype\n",
    "\n",
    "colname = []\n",
    "\n",
    "for x in adult_df_rev.columns:\n",
    "    if adult_df_rev[x].dtypes=='object':\n",
    "        colname.append(x)\n",
    "\n",
    "print('Columns with \"object\" as their data type','\\n', colname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2d83d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "\n",
    "for x in colname:\n",
    "    adult_df_rev[x]=le.fit_transform(adult_df_rev[x])\n",
    "    le_name_mapping = dict(zip(le.classes_, le.transform(le.classes_)))\n",
    "    print('Feature', x)\n",
    "    print('mapping', le_name_mapping)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb20f70b",
   "metadata": {},
   "source": [
    "All the categoric values have been converted into numeric data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c53a9e",
   "metadata": {},
   "source": [
    "Target variable labels\n",
    "\n",
    "Feature : income<br>\n",
    "{'<=50K' :  0,  '>50K' :  1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4867ad08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking if the data has been converted\n",
    "\n",
    "adult_df_rev.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd886b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "adult_df_rev.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d372c9",
   "metadata": {},
   "source": [
    "The categoric data have been converted successfully<br>\n",
    "All the data is present in integer data type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f3baa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create X and Y variable (Independent - X and Dependent - Y)\n",
    "\n",
    "X = adult_df_rev.values[:,0:-1]\n",
    "Y = adult_df_rev.values[:,-1]\n",
    "\n",
    "print('X :',X)\n",
    "print('Y :',Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde2c3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking if the X and Y variables are created properly\n",
    "\n",
    "print('X = ',X.shape)\n",
    "print('Y = ',Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71fcbea",
   "metadata": {},
   "source": [
    "The X or independent variable has 29096 rows and 12 columns<br>\n",
    "The Y or dependent or target variable has 29096 rows and 1 column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7aa8ddb",
   "metadata": {},
   "source": [
    "#### **Scaling the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88508d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using standardozation scaling technique to scale the data\n",
    "\n",
    "# For X variable\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "scaler.fit(X)\n",
    "X = scaler.transform(X)\n",
    "\n",
    "print('X variable','\\n',X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70064e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Y variable\n",
    "\n",
    "Y = Y.astype(int)\n",
    "Y.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bde4b3e",
   "metadata": {},
   "source": [
    "#### **Basic models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a4f6ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data for Training and Testing\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.3, random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71a3cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Data reserved for Training = 70%')\n",
    "print()\n",
    "print('Training data shape for X variable - ', X_train.shape) \n",
    "print('Training data shape for Y variable - ',Y_train.shape)  \n",
    "print()\n",
    "print('Data reserved for Testing = 30%')\n",
    "print()\n",
    "print('Testing data shape for X variable - ',X_test.shape)\n",
    "print('Testing data shape for Y variable - ',Y_test.shape)   \n",
    "print()\n",
    "print(\"Percent of train data = \",X_train.shape[0]/X.shape[0]*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14786dd",
   "metadata": {},
   "source": [
    "#### **Logistic Regression Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b61ef6",
   "metadata": {},
   "source": [
    "\n",
    "Logistic regression is a supervised machine learning algorithm mainly used for classification tasks where the goal is to predict the probability that an instance of belonging to a given class. It is used for classification algorithms its name is logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8793e43a",
   "metadata": {},
   "source": [
    "We have decided to implement Logistic regression here as the data shows binary class classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2024d422",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Create a model\n",
    "classifier=LogisticRegression()\n",
    "\n",
    "# Fitting training data to the model\n",
    "classifier.fit(X_train,Y_train)\n",
    "\n",
    "Y_pred=classifier.predict(X_test)\n",
    "print(Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd98a7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(zip(Y_test,Y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad85cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Beta coefficient','\\n')\n",
    "print(list(zip(adult_df_rev.columns[:-1],classifier.coef_.ravel())))\n",
    "print()\n",
    "print('Intercept/Beta0','\\n',classifier.intercept_)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac6324f",
   "metadata": {},
   "source": [
    "* From the above we can infer the relation between the Independent and dependent variable "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb2cb44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the probability of the data to belong to either class 0 or class 1\n",
    "\n",
    "y_pred_class=classifier.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0ece69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation matrix\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "\n",
    "cfm = confusion_matrix(Y_test,Y_pred)\n",
    "\n",
    "print('Confusion Matrix','\\n',cfm)\n",
    "print()\n",
    "print('Classification Report','\\n',classification_report(Y_test,Y_pred))\n",
    "print()\n",
    "print('Accuracy of the model -', accuracy_score(Y_test,Y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa07c4a",
   "metadata": {},
   "source": [
    "From the above we can infer that-<br>\n",
    "1. Class 0 : Out of 6550 values 6178 were predicted correctly while 372 were misclassified<br>\n",
    "2. Class 1 : Out of 2179 vales 968 were predicted correctly while 1211 were misclassified\n",
    "\n",
    "Recall values:<br>\n",
    "1. Class 0 : 94%<br>\n",
    "2. Class 1 : 44%\n",
    "\n",
    "Hence the model is working well for Class 0 and not so well for Class 1\n",
    "\n",
    "The accuracy of the model is 81.86%\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c263e66",
   "metadata": {},
   "source": [
    "The model still needs some work done, hence we will tune it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b21ea42",
   "metadata": {},
   "source": [
    "#### **Tuning the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97039c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in np.arange(0.4,0.61,0.01):\n",
    "    predict_mine = np.where(y_pred_class[:,1]>x,1,0)\n",
    "    cfm=confusion_matrix(Y_test,predict_mine)\n",
    "    total_err=cfm[0,1]+cfm[1,0]\n",
    "    print('Errors at threshold', x,':',total_err, \", type 2 error :\", cfm[1,0],' type 1 error', cfm[0,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "602cd27b",
   "metadata": {},
   "source": [
    "The best threshold value : 0.4600000000000001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e10f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty list\n",
    "y_pred_class_final=[]\n",
    "\n",
    "for value in y_pred_class[:,1]:\n",
    "    if value >0.46:\n",
    "        y_pred_class_final.append(1)\n",
    "    else:\n",
    "        y_pred_class_final.append(0)\n",
    "        \n",
    "print(y_pred_class_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b2ec962",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfm = confusion_matrix(Y_test,y_pred_class_final)\n",
    "print('Confusion Matrix','\\n',cfm)\n",
    "print()\n",
    "print('Previous Confusion Matrix')\n",
    "print('[[6178  372]')\n",
    "print('[1211  968]]')\n",
    "print()\n",
    "print('Classification Report','\\n',classification_report(Y_test,y_pred_class_final))\n",
    "print()\n",
    "print('Accuracy of the model -', accuracy_score(Y_test,y_pred_class_final))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49306a04",
   "metadata": {},
   "source": [
    "From the above we can infer that-<br>\n",
    "1. Class 0 : Out of 6550 values 6094 were predicted correctly while 456 were misclassified<br>\n",
    "2. Class 1 : Out of 2179 vales 1126 were predicted correctly while 1053 were misclassified\n",
    "\n",
    "Recall values:<br>\n",
    "1. Class 0 : 93%<br>\n",
    "2. Class 1 : 48%\n",
    "\n",
    "\n",
    "The accuracy of the model is 81.87%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43969b8d",
   "metadata": {},
   "source": [
    "#### **Conclusion**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9485094",
   "metadata": {},
   "source": [
    "Hence we have built a model to predict the income of various people based on different factors like ' of 81.87% accuracy, using Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe408af",
   "metadata": {},
   "source": [
    "#### Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a8ed7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the test data\n",
    "\n",
    "adult_test = pd.read_csv(r\"C:\\Users\\Anny\\OneDrive\\Desktop\\Imarticus\\GITHUB\\Adult\\adult_test (2).csv\",header=None, delimiter = ' *, *')\n",
    "\n",
    "adult_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d58912",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inserting column names\n",
    "\n",
    "adult_test.columns = ['age', 'workclass', 'fnlwgt', 'education', 'education_num',\n",
    "                    'marital_status', 'occupation', 'relationship',\n",
    "                    'race', 'sex', 'capital_gain', 'capital_loss',\n",
    "                    'hours_per_week', 'native_country', 'income']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3a8eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking if column names have been updated or not\n",
    "\n",
    "adult_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465eae7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Selection\n",
    "\n",
    "adult_test.drop(['education','fnlwgt'],axis = 1,inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c06dd271",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking if the columns have been dropped\n",
    "\n",
    "adult_test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0259433c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for missing values in the data\n",
    "\n",
    "adult_test.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee7ef07",
   "metadata": {},
   "source": [
    "There is no missing values in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5985d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for anomalies in the data types "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb4bbe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "adult_test.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce78efdb",
   "metadata": {},
   "source": [
    "A few of the numeric columns have object as their data types, which indicates some kind of anomaly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd56502",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking if the data has '?' as missing values in the data\n",
    "\n",
    "for x in adult_test.columns:\n",
    "    print({x:adult_test[x].unique()})\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "744ab5c3",
   "metadata": {},
   "source": [
    "Work class, occupation and native country have '?' as their missing value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5867c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting special charecter '?' into nan values\n",
    "\n",
    "adult_test.replace('?',np.nan,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9750f9d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking if the the special charecter has been converted into nan values\n",
    "\n",
    "adult_test.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad92328",
   "metadata": {},
   "source": [
    "There are missing values in workclass, occupation and native country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2848a18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "adult_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2810834c",
   "metadata": {},
   "outputs": [],
   "source": [
    "adult_test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe983b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filling the missing data with the mode values\n",
    "\n",
    "for value in ['workclass','occupation','native_country']:\n",
    "    adult_test[value].fillna(adult_test[value].mode()[0],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41583f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking if the missing values have been filled \n",
    "\n",
    "adult_test.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5bbcf8",
   "metadata": {},
   "source": [
    "All the missing values have been filled successfully with their respective modes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f454809",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a list with all categoric columns\n",
    "\n",
    "categoric = []\n",
    "\n",
    "for i in adult_test.columns:\n",
    "    if adult_test[i].dtypes=='object':\n",
    "        categoric.append(i)\n",
    "    \n",
    "categoric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c252ad4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting all the categoric data into numeric data\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "\n",
    "for i in categoric:\n",
    "    adult_test[i]=le.fit_transform(adult_test[i])\n",
    "    \n",
    "    name = dict(zip(le.classes_,le.transform(le.classes_)))\n",
    "    print('Feature:',x)\n",
    "    print('Mapping', name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0916b9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "adult_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ecbba4",
   "metadata": {},
   "source": [
    "All the categoric columns have been converted into numeric values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d5c8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating new X and Y variables\n",
    "\n",
    "X_test_new = adult_test.values[:,:-1]\n",
    "Y_test_new = adult_test.values[:,-1]\n",
    "\n",
    "print('New X variable :', X_test_new)\n",
    "print('New Y variable :', Y_test_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337ff10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling the data\n",
    "\n",
    "# Scaling the X variable\n",
    "\n",
    "X_test_new = scaler.transform(X_test_new)\n",
    "print(X_test_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9975ab03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling the Y variable\n",
    "\n",
    "Y_test_new = Y_test_new.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c616f69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting the probabilty of the values to belong to either class 0 or class 1\n",
    "\n",
    "Y_pred_prob = classifier.predict_proba(X_test_new)\n",
    "print(Y_pred_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f60218",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred_class_test=[]\n",
    "\n",
    "for value in Y_pred_prob[:,1]:\n",
    "    if value >0.46:\n",
    "        Y_pred_class_test.append(1)\n",
    "    else:\n",
    "        Y_pred_class_test.append(0)\n",
    "        \n",
    "print(Y_pred_class_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3e30dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating the model \n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report, recall_score, f1_score\n",
    "\n",
    "# Functions - Evaluation matrix\n",
    "\n",
    "cfm = confusion_matrix(Y_test_new,Y_pred_class_test)\n",
    "\n",
    "print('Confusion Matrix','\\n', cfm)\n",
    "print()\n",
    "print('Classification report: ','\\n')\n",
    "print()\n",
    "print(classification_report(Y_test_new,Y_pred_class_test))\n",
    "print()\n",
    "acc= accuracy_score(Y_test_new,Y_pred_class_test)\n",
    "print()\n",
    "print('Accuracy of the model = ', acc)\n",
    "print(acc*100,'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070ff55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparing validation accuracy with test data accuracy\n",
    "# Model is durable as the range is between 80-83%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a6b3e55",
   "metadata": {},
   "source": [
    "Looking at the accuracy score one might infer that the model is a good model, but looking closely at the recall value for class 1, we can say that the model is not the best and can be improved further by applying different algorithms."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
